
<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Scraped Content from https://www.cybercrowd.co.uk/news/dont-assume-ai-is-secure-lessons-from-a-recent-penetration-test</title>
<source_url>https://www.cybercrowd.co.uk/news/dont-assume-ai-is-secure-lessons-from-a-recent-penetration-test</source_url>
</head>
<body>
News AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Here’s how to start: Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd AI adoption is skyrocketing. From customer service bots to productivity assistants, organisations are racing to embed generative AI into their workflows – often building on trusted platforms like Microsoft Azure OpenAI or OpenAI’s own API. These foundations are powerful and, on paper, secure. But as we recently discovered during a penetration test for a client’s private ChatGPT-style application, “secure by design” doesn’t mean “secure in practice.” The Comfort of Big Names Our client had created a private AI chat platform – built on OpenAI’s large language model (LLM) and fully integrated into their Microsoft environment. With the strength of two industry leaders behind it, the team felt confident: Their data was hosted on Microsoft’s secure infrastructure Their identity and access management policies were enforced Their AI instance was “private,” so no data could escape In theory, it looked watertight. In practice, our testing uncovered significant weaknesses. When Configuration Undermines Confidence Our penetration tester, Alberto, was able to identify and exploit several vulnerabilities that undermined the application’s security model. These weren’t flaws in Microsoft or OpenAI’s platforms themselves – they were gaps in configuration, integration, and governance . This is a critical distinction. Major providers offer the building blocks for secure AI adoption, but it’s up to each organisation to assemble them correctly . The most common pitfalls we see include: Misconfigured access controls – where employees or service accounts have broader permissions than intended Inadequate data segregation – allowing sensitive information to move across tenants or between production and testing environments Weak prompt injection protections – enabling attackers to manipulate inputs to access hidden data or override model instructions Poor API security – with exposed keys, unencrypted traffic, or lack of rate limiting and anomaly detection No change control for AI configurations – meaning new model updates, API calls, or integrations aren’t revalidated Limited visibility and logging – making it hard to detect prompt abuse, data exfiltration, or policy violations AI systems amplify the consequences of small misconfigurations. A single overlooked permission or unmonitored API can expose data, models, or entire workflows. Steps to Strengthen Your AI Security Securing AI isn’t about reinventing your cybersecurity strategy – it’s about applying strong fundamentals to a new and fast-moving landscape. Conduct targeted penetration testing Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Test not just the app layer, but also the AI logic, prompt structures, and API endpoints. Validate that model integrations don’t leak system prompts or confidential data. Harden identity and access management (IAM) Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Enforce least-privilege access and strong conditional access policies. Regularly review service principals and token permissions – AI integrations often overreach. Protect sensitive data Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Classify and label data before it enters AI workflows. Use data masking or pseudonymisation for prompts and logs. Keep AI interaction logs secure – they can contain hidden personal or business-sensitive data. Monitor AI activity in real time Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Track query patterns and output anomalies that may signal prompt injection or misuse. Integrate monitoring into your existing SIEM or SOC workflows. Secure your APIs and model endpoints Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Rotate keys regularly, enforce TLS, apply rate limiting, and monitor API health. Consider implementing an API gateway with built-in inspection for malicious or overreaching requests. Embed governance and testing into your AI lifecycle Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Treat AI models as dynamic systems – review and revalidate them after updates or configuration changes. Incorporate AI-specific risk assessments into your DevSecOps or MLOps pipelines. Final Thought: Secure AI Starts with Secure Thinking AI isn’t just another application – it’s a new layer of complexity that touches data, identity, and decision-making. And while Microsoft, OpenAI, and others offer robust foundations, security doesn’t live in the platform – it lives in your implementation . The rapid pace of AI adoption means many organisations are deploying powerful tools faster than they’re securing them. Without continuous validation, testing, and monitoring, these systems can quietly become new attack surfaces. At CyberCrowd, we help organisations bridge that gap – from AI-specific penetration testing and configuration reviews to governance frameworks and incident response readiness. Before you trust your AI, test your AI. Get in touch with our team to discuss how we can help your organisation deploy and defend AI securely. Related Read: Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters Cloud Doesn’t Automatically Mean Resilient: Why Testing Matters – CyberCrowd Related Posts News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... 1 February 2026 News Pen Testing LEARN MORE Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security Thinking Like an Attacker: Why Penetration Testing Still Defines Strong Cyber Security In a cyber security landscape dominated by automation, dashboards, and alerts, one truth remains constant:... News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... 27 January 2026 News LEARN MORE Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have Cyber Security in 2026: The Risks Haven’t Changed – But the Stakes Have As we move into 2026, cyber security headlines feel familiar.Ransomware. Data breaches. Supply chain risk.... News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... 17 December 2025 News LEARN MORE Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Delivering Realistic Cyber Incident Tabletop Exercises That Build True Organisational Resilience Article by Nigel Plant, Senior Security Consultant, CyberCrowd. Over the past month CyberCrowd delivered four... News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,... 25 October 2025 News LEARN MORE Careful Where You Click: How to Spot & Report Scams Careful Where You Click: How to Spot & Report Scams October is Cybersecurity Awareness Month, and as we wrap up our Core 4 series (passwords,...
</body>
</html>
